\documentclass{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{bbm} % Pour \mathbbm{1}

\title{Clustered Multilevel Monte Carlo (C-MLMC)}
\date{\today}

\begin{document}
\maketitle

\section{Problem Setup and Notation}
We aim to estimate \( \mathbb{E}[P] \) for a path-dependent payoff \( P \) under an SDE model. Using the Multilevel Monte Carlo (MLMC) framework, we define level-wise differences:
\[
\Delta P_\ell =
\begin{cases}
P_0, & \text{if } \ell = 0, \\
P_\ell - P_{\ell-1}, & \text{if } \ell \ge 1.
\end{cases}
\]
Let \( V_\ell = \mathrm{Var}[\Delta P_\ell] \), and \( C_\ell \) be the average cost per sample on level \( \ell \). Classical MLMC chooses:
\[
N_\ell \propto \sqrt{\frac{V_\ell}{C_\ell}}
\]
to meet the MSE constraint:
\[
\sum_{\ell=0}^L \frac{V_\ell}{N_\ell} \le (1 - \theta)\varepsilon^2.
\]

\section{Motivation for Clustering}
When the variance is concentrated in rare modes of the input space, uniform sampling becomes inefficient. C-MLMC improves efficiency by:
\begin{itemize}
  \item Extracting features from simulation details,
  \item Clustering samples via K-means into \( n_c \) variance modes,
  \item Estimating per-cluster statistics \( (P_{\ell,c}, V_{\ell,c}) \),
  \item Allocating samples proportionally to \( \sqrt{P_{\ell,c} V_{\ell,c}} \).
\end{itemize}

\begin{algorithm}[H]
    \caption{Stratified Clustered MLMC Estimator}
    \label{alg:strat-cmlmc}
    \begin{algorithmic}[1]
    \REQUIRE Simulator \( f(\ell, N, \texttt{return\_details}) \), feature extractor \( \varphi \), tolerance \( \varepsilon \), pilot size \( N_0 \), number of clusters \( n_c \)
    \STATE Initialize level count \( L \leftarrow L_{\min} \), precision parameter \( \theta \in (0,1) \)
    \FOR{each level \( \ell \in \{0, \dots, L\} \)}
        \STATE Run pilot: simulate \( \{(Y_i, d_i)\}_{i=1}^{N_0} \leftarrow f(\ell, N_0, \texttt{true}) \)
        \IF{\( \ell > 0 \)}
            \STATE Extract features: \( \phi_i \leftarrow \varphi(d_i) \)
            \STATE Cluster \( \{\phi_i\} \) into \( n_c \) clusters using K-means
            \STATE For each cluster \( c \in \{1,\dots,n_c\} \), compute:
            \[
            P_{\ell,c} = \frac{\sum_{i=1}^{N_0} \mathbbm{1}_{\{\phi_i \in c\}}}{N_0}, \quad
            V_{\ell,c} = \mathrm{Var}\left(Y_i \mid \phi_i \in c\right)
            \]
        \ELSE
            \STATE Estimate global variance: \( V_0 = \mathrm{Var}(Y_i) \)
        \ENDIF
    \ENDFOR
    
    \WHILE{not converged}
        \FOR{each level \( \ell > 0 \)}
            \STATE Compute per-cluster sample allocations:
            \[
            N_{\ell,c} \propto P_{\ell,c} \sqrt{V_{\ell,c}}, \quad \text{and normalize such that } \sum_{c=1}^{n_c} N_{\ell,c} = N_\ell
            \]
            \STATE Estimate cost per sample \( C_\ell \gets \texttt{estimated from runtime or set a priori} \)
            \STATE Compute aggregated variance: \( V_\ell = \sum_{c=1}^{n_c} \frac{P_{\ell,c}^2 V_{\ell,c}}{N_{\ell,c}} \)
        \ENDFOR
        \STATE Compute total variance target:
        \[
        \sum_{\ell=0}^L V_\ell \le \theta^2 \varepsilon^2
        \]
        \STATE For level \( \ell = 0 \): increase \( N_0 \) if needed and update \( V_0 \)
        \STATE Estimate total bias from finest levels (e.g., regression fit \( |\mathbb{E}[\Delta P_\ell]| \sim \mathcal{O}(2^{-\alpha \ell}) \))
        \IF{estimated bias \( > (1 - \theta)^2 \varepsilon^2 \)}
            \STATE Increase level count: \( L \leftarrow L + 1 \), and perform pilot on new level
        \ELSE
            \STATE \textbf{break}
        \ENDIF
    \ENDWHILE
    
    \FOR{each level \( \ell \)}
        \IF{\( \ell > 0 \)}
            \FOR{each cluster \( c \)}
                \STATE Sample \( N_{\ell,c} \) new paths with cluster label \( c \)
                \STATE Compute sample mean: \( \hat{\mu}_{\ell,c} = \frac{1}{N_{\ell,c}} \sum Y_i^{(c)} \)
            \ENDFOR
            \STATE Compute level mean:
            \[
            \hat{\mu}_\ell = \sum_{c=1}^{n_c} P_{\ell,c} \hat{\mu}_{\ell,c}
            \]
        \ELSE
            \STATE Estimate level mean: \( \hat{\mu}_0 = \frac{1}{N_0} \sum_{i=1}^{N_0} Y_i \)
        \ENDIF
    \ENDFOR
    
    \STATE \textbf{Return:} Estimator \( \hat{P} = \sum_{\ell=0}^L \hat{\mu}_\ell \)
    \end{algorithmic}
    \end{algorithm}

\end{document}
