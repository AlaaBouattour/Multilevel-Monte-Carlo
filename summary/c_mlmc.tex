\documentclass{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{hyperref}

\title{Clustered Multilevel Monte Carlo (C-MLMC)}
\date{\today}

\begin{document}
\maketitle

\section{Problem Setup and Notation}
We aim to estimate \( \mathbb{E}[P] \) for a path-dependent payoff \( P \), evaluated under an SDE model. Using the Multilevel Monte Carlo (MLMC) framework, we define estimators:
\[
\Delta P_\ell =
\begin{cases}
P_0, & \text{if } \ell = 0, \\
P_\ell - P_{\ell-1}, & \text{if } \ell \ge 1.
\end{cases}
\]
Let \( V_\ell = \mathrm{Var}[\Delta P_\ell] \), and \( C_\ell \) be the average cost per sample on level \( \ell \). Classical MLMC chooses
\[
N_\ell \propto \sqrt{\frac{V_\ell}{C_\ell}}
\]
to achieve RMSE \( \varepsilon \), with:
\[
\sum_{\ell=0}^L \frac{V_\ell}{N_\ell} \le (1 - \theta)\varepsilon^2.
\]

\section{Motivation for Clustering}
When the variance is highly concentrated in rare regions of the input space (e.g., digital or exotic options), naive sampling wastes budget on low-variance samples. The C-MLMC approach improves efficiency by:
\begin{itemize}
  \item Extracting features from simulation metadata,
  \item Clustering samples to identify variance modes,
  \item Estimating per-cluster variance \( V_{\ell,c} \) and probabilities \( P_{\ell,c} \),
  \item Allocating samples in proportion to \( \sqrt{P_{\ell,c} V_{\ell,c}} \).
\end{itemize}

\section{C-MLMC Algorithm}
\begin{algorithm}[H]
\caption{Clustered MLMC Estimator}
\label{alg:cmlmc}
\begin{algorithmic}[1]
\REQUIRE Simulator \( f(\ell, N, \texttt{return\_details}) \), feature extractor \( \varphi \), tolerance \( \varepsilon \), pilot size \( N_0 \), clusters \( n_c \)
\STATE Initialize \( L \leftarrow L_{\min},\;\theta \in (0,1) \)
\FOR{each level \( \ell \le L \)}
    \STATE Run pilot: \( \{(Y_i, d_i)\} \leftarrow f(\ell, N_0, \texttt{true}) \)
    \STATE Compute features \( \phi_i = \varphi(d_i) \)
    \STATE Cluster features using K-means into \( n_c \) clusters
    \STATE Estimate cluster variances \( V_{\ell,c} \) and probabilities \( P_{\ell,c} \)
\ENDFOR
\WHILE{not converged}
    \STATE Compute level variances \( V_\ell = \sum_c P_{\ell,c} V_{\ell,c} \)
    \STATE Estimate level costs \( C_\ell \)
    \STATE Allocate samples \( N_\ell \propto \sqrt{V_\ell / C_\ell} \)
    \STATE Allocate cluster-wise \( N_{\ell,c} \propto \sqrt{P_{\ell,c} V_{\ell,c}} \)
    \FOR{each cluster \( c \)}
        \STATE Sample until \( N_{\ell,c} \) accepted samples with cluster label \( c \)
        \STATE Accumulate \( \sum Y \), \( \sum Y^2 \), update cost
    \ENDFOR
    \STATE Bias check via extrapolation; if too large, increment \( L \)
\ENDWHILE
\STATE Output estimate \( \hat{P} = \sum_{\ell=0}^L \frac{1}{N_\ell} \sum_{i=1}^{N_\ell} Y_{\ell,i} \)
\end{algorithmic}
\end{algorithm}

\section{Why \(\sqrt{P_c V_c}\) Allocation?}
To minimize estimator variance:
\[
\mathrm{Var}(\hat{Y}) = \sum_c \frac{P_c^2 V_c}{N_c}
\quad \text{subject to} \quad \sum_c N_c = N.
\]
Using Lagrange multipliers, the optimal allocation satisfies:
\[
N_c \propto P_c \sqrt{V_c} \quad \Rightarrow \quad \text{weights} \propto \sqrt{P_c V_c}.
\]

\section{Unbiasedness}
Each level estimator:
\[
\hat{Y}_\ell = \frac{1}{N_\ell} \sum_{i=1}^{N_\ell} Y_{\ell,i}
\]
is an unbiased estimate of \( \mathbb{E}[\Delta P_\ell] \), since stratified sampling preserves marginal distributions. The full estimator:
\[
\hat{P} = \sum_{\ell=0}^L \hat{Y}_\ell
\]
is unbiased for \( \mathbb{E}[P_L] \), with bias \( \mathbb{E}[P - P_L] \) controlled via extrapolation.

\section{Conclusion}
C-MLMC enhances classical MLMC by identifying and focusing computation on variance-dominant clusters. The allocation \( \propto \sqrt{P_c V_c} \) provides a provably efficient strategy to reduce variance while retaining unbiasedness and convergence guarantees. The result is a more robust and scalable Monte Carlo estimator for path-dependent problems.

\end{document}
